# Alignment Faking Detection - Experiment Tracker

## Status Matrix

```
                              â”‚ Gemma-3-27B â”‚ Llama-3.1-8B â”‚ Qwen3-14B â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  DETECTION                   â”‚             â”‚              â”‚           â”‚
    AF Detector Training      â”‚     âœ…      â”‚      âœ…      â”‚    ðŸ”²     â”‚
    Gold-106 Evaluation       â”‚     âœ…      â”‚      âœ…      â”‚    ðŸ”²     â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  CAUSAL - ABLATION           â”‚             â”‚              â”‚           â”‚
    Single Feature Ablation   â”‚     âœ…      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
    Multi-Feature Ablation    â”‚     âœ…      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
    Activation Patching       â”‚     âœ…      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  CAUSAL - STEERING           â”‚             â”‚              â”‚           â”‚
    Classification Steering   â”‚     ðŸ”²      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
    Generation Steering       â”‚     ðŸ”²      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
    Proper Steering Vectors   â”‚     ðŸ”²      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  MECHANISTIC                 â”‚             â”‚              â”‚           â”‚
    Attention Analysis        â”‚     ðŸ”²      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
    Layer Probing             â”‚     ðŸ”²      â”‚      ðŸ”²      â”‚    ðŸ”²     â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Progress

```
  Gemma-27B:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50%
  Llama-8B:   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  20%
  Qwen3-14B:  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
```

## Notes

### Gemma-3-27B (Lead Model)
- **Detector**: gemma3-27b-af-detector-v2 on HuggingFace
- **SAE**: GemmaScope 2-27B-IT, best at layer 40
- **Peak AUROC**: 0.83 on Gold-106
- **Ablation results**: H100 experiments completed 2026-01-05

### Llama-3.1-8B
- **Detector**: llama-8b-af-detector (v4) on HuggingFace
- **SAE**: LlamaScope lxm_32x (131K features)
- **Peak AUROC**: 0.72 (direct classifier), 0.68 (SAE features at L0)
- **Finding**: LlamaScope SAE ceiling ~0.68 vs GemmaScope 0.83
- **Next**: Run causal ablation experiments with LlamaScope

### Qwen3-14B
- **Status**: Blocked on detector training
- **Note**: Revisit later once proper trained model figured out

## Key Findings

1. **GemmaScope outperforms LlamaScope** for AF detection (0.83 vs 0.68 AUROC)
2. **Layer 0 best for LlamaScope** - embedding layer most discriminative (surprising)
3. **Deep layers near random** for Llama (L24, L28 ~0.45-0.50 AUROC)
4. **Training improves early layers** but hurts later layers in Llama

## File Locations

- **BoxGlass**: `nigel:~/boxglass/2026-01-05/`
- **H100 Backup**: `~/boxglass/2026-01-05/h100_backup/`
- **HuggingFace**: vincentoh/alignment-faking-training (datasets)

---
Last updated: 2026-01-05
