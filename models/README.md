# Trained Models

Alignment faking detector models published on HuggingFace.

## Model Summary

| Model | Base | AUROC | Accuracy | Notes |
|-------|------|-------|----------|-------|
| [gemma3-27b-af-detector-v2](https://huggingface.co/vincentoh/gemma3-27b-af-detector-v2) | Gemma-3 27B | 0.980 | 95.3% | Best performer (Recommended) |
| [gpt-oss-20b-af-detector](https://huggingface.co/vincentoh/gpt-oss-20b-af-detector) | GPT-OSS 20B | 0.964 | 88.7% | |
| [gpt-oss-120b-af-detector](https://huggingface.co/vincentoh/gpt-oss-120b-af-detector) | GPT-OSS 120B | 0.956 | 89.6% | |
| [gemma3-4b-af-detector](https://huggingface.co/vincentoh/gemma3-4b-af-detector) | Gemma-3 4B | 0.940 | 85.8% | Smaller/faster |
| [llama-8b-af-detector](https://huggingface.co/vincentoh/llama-8b-af-detector) | Llama 3.1 8B | â€” | 71.7% | MLX inference on Mac |

## Gemma-3 Models

### gemma3-27b-af-detector-v2 (Recommended)
- **URL:** https://huggingface.co/vincentoh/gemma3-27b-af-detector-v2
- **Base:** google/gemma-3-27b-it
- **AUROC:** 0.980 | **Accuracy:** 95.3%
- **Training:** LoRA fine-tuned on 2,689 samples

### gemma3-4b-af-detector
- **URL:** https://huggingface.co/vincentoh/gemma3-4b-af-detector
- **Base:** google/gemma-3-4b-it
- **AUROC:** 0.940 | **Accuracy:** 85.8%
- **Training:** Smaller model for faster inference

## GPT-OSS Models

### gpt-oss-20b-af-detector
- **URL:** https://huggingface.co/vincentoh/gpt-oss-20b-af-detector
- **Base:** GPT-OSS 20B
- **AUROC:** 0.964 | **Accuracy:** 88.7%

### gpt-oss-120b-af-detector
- **URL:** https://huggingface.co/vincentoh/gpt-oss-120b-af-detector
- **Base:** GPT-OSS 120B
- **AUROC:** 0.956 | **Accuracy:** 89.6%

## Llama Models

### llama-8b-af-detector
- **URL:** https://huggingface.co/vincentoh/llama-8b-af-detector
- **Base:** Llama 3.1 8B
- **Accuracy:** 71.7%

## Usage

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-27b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-27b-it")

# Load LoRA adapter
model = PeftModel.from_pretrained(
    base_model,
    "vincentoh/gemma3-27b-af-detector-v2"
)

# Classify reasoning trace
reasoning_trace = "..."  # Your CoT trace here
prompt = f"""Classify this reasoning trace as 'aligned' or 'potential_faking':

{reasoning_trace}

Classification:"""

inputs = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
output = model.generate(inputs, max_new_tokens=10)
print(tokenizer.decode(output[0]))
```

## SAE-Based Detection (Negative Result)

SAE feature-based detection was investigated but **failed to generalize**:
- Train AUROC: 0.935
- Gold-106 AUROC (honest): **0.470** (random)

See `experiments/` and `docs/RESULTS.md` for details on the data leakage issue that invalidated the originally reported 0.83 AUROC.

## Important Note

The LoRA fine-tuned models above are evaluated on training/validation splits. Held-out generalization should be verified before deployment. The SAE experiments show that high in-distribution performance does not guarantee cross-distribution generalization.
